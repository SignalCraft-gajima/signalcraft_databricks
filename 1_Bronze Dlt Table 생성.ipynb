{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a8cbdc5-a684-47f4-8ba7-675c9a22e229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: signalcraft_databricks.default.bronze_netflix  (rows=8809)\n✅ Saved: signalcraft_databricks.default.bronze_user  (rows=10000)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "BASE_PATH = \"/Volumes/signalcraft_databricks/signalcraft_test/bronze\"\n",
    "CATALOG = \"signalcraft_databricks\"\n",
    "SCHEMA  = \"default\"\n",
    "\n",
    "NETFLIX_FILE = \"netflix.csv\"\n",
    "USER_FILE    = \"user.csv\"\n",
    "\n",
    "NETFLIX_TABLE = f\"{CATALOG}.{SCHEMA}.bronze_netflix\"\n",
    "USER_TABLE    = f\"{CATALOG}.{SCHEMA}.bronze_user\"\n",
    "\n",
    "# =========================\n",
    "# 1) netflix.csv -> bronze_netflix\n",
    "# =========================\n",
    "netflix_path = f\"{BASE_PATH}/{NETFLIX_FILE}\"\n",
    "\n",
    "df_netflix = (\n",
    "    spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(netflix_path)\n",
    "      .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "      .withColumn(\"_source_file\", F.lit(netflix_path))\n",
    ")\n",
    "\n",
    "(df_netflix.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")     # 최초 적재: overwrite / 이후 누적이면 append 고려\n",
    " .saveAsTable(NETFLIX_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"✅ Saved: {NETFLIX_TABLE}  (rows={df_netflix.count()})\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) user.csv -> bronze_user\n",
    "# =========================\n",
    "user_path = f\"{BASE_PATH}/{USER_FILE}\"\n",
    "\n",
    "df_user = (\n",
    "    spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(user_path)\n",
    "      .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "      .withColumn(\"_source_file\", F.lit(user_path))\n",
    ")\n",
    "\n",
    "(df_user.write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(USER_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"✅ Saved: {USER_TABLE}  (rows={df_user.count()})\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5940318638326061,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Bronze Dlt Table 생성",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}