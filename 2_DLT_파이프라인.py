import dlt
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType

# =========================================================
# ğŸ“Œ SIGNALCRAFT OTT CHURN PREVENTION PIPELINE (DLT)
# =========================================================
# ëª©ì :
#   1) ì‹œì²­ ì´ë²¤íŠ¸(ê³¼ê±° Delta + ì‹¤ì‹œê°„ Capture)ë¥¼ í†µí•©í•´ ì¼ë³„ ì‹œì²­ì‹œê°„ì„ ë§Œë“ ë‹¤.
#   2) ìœ ì € í–‰ë™ ìŠ¤ëƒ…ìƒ·ì„ ìƒì„±í•˜ê³ (T-1 í™•ì •ë³¸), ì„œë¹„ìŠ¤/ì´íƒˆ KPI ë° ë¦¬í…ì…˜ì„ ë§Œë“ ë‹¤.
#   3) ë§ˆì¼€íŒ… ì „ëµ í™•ì • í…Œì´ë¸”(dlt_gold_campaign_targets)ì„ ìƒì„±í•œë‹¤.
#
# ë ˆì´ì–´:
#   - Bronze: ì›ì²œ(íŒŒì¼/ìº¡ì²˜) ë¡œë“œ, JSON íŒŒì‹±, ì˜¤ë¥˜ ê²©ë¦¬
#   - Silver: ì´ë²¤íŠ¸ í†µí•©, ì¼ë³„ ì§‘ê³„, Full Matrix(ë¯¸ì ‘ì†=0 í¬í•¨)
#   - Gold  : Snapshot / KPI / Retention / Campaign Targets
#
# í•µì‹¬ ì›ì¹™:
#   - ìš´ì˜ ê°€ì •(T-1 ìŠ¤ëƒ…ìƒ·):
#       ì˜¤ëŠ˜ ë“¤ì–´ì˜¨ ë°ì´í„°ëŠ” ì™„ì „í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
#       "ë°ì´í„° ê¸°ì¤€ max(event_date) - 1"ì„ í™•ì •ë³¸ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
#   - Campaign TargetsëŠ” RAG ì…ë ¥ SSOT:
#       strategy_code / priority_rank / send_flag ê¸°ì¤€ìœ¼ë¡œ
#       ë©”ì‹œì§€ ìƒì„±(RAG)ì´ ì´ë¤„ì§„ë‹¤.
#       (RAG ê²°ê³¼ ë¡œê·¸ëŠ” Goldë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê¸° ìœ„í•´ Silverë¡œ ì €ì¥ ê¶Œì¥)
# =========================================================


# =========================================================
# 0) ê³µí†µ ì„¤ì •
# =========================================================
CATALOG = "signalcraft_databricks"
SCHEMA  = "default"

# ì›ì²œ ë°ì´í„° ì†ŒìŠ¤ (Storage)
USER_SRC  = "abfss://signalcraft-data@signalcraftstorage.dfs.core.windows.net/user/"
CONT_SRC  = "abfss://signalcraft-data@signalcraftstorage.dfs.core.windows.net/contents/"

# ê³¼ê±°(1ë…„ì¹˜) ì›ì²œ ë¡œê·¸ Delta í…Œì´ë¸”
HISTORY_TABLE = f"{CATALOG}.{SCHEMA}.bronze_watch_history"

# Event Hubs Capture (AVRO) ì†ŒìŠ¤ ê²½ë¡œ
WATCH_CAPTURE_SRC = "abfss://signalcraft-data@signalcraftstorage.dfs.core.windows.net/signalcraft-eventhub/http-events/"
PAYLOAD_COL = "Body"

# âœ… ëª¨ë¸ ê²°ê³¼(SSOT) í…Œì´ë¸”
MODEL_PRED_TABLE = f"{CATALOG}.{SCHEMA}.gold_churn_predictions"


# =========================================================
# 0-1) ê³µí†µ ìŠ¤í‚¤ë§ˆ/ìœ í‹¸ í•¨ìˆ˜
# =========================================================

# Capture payload(JSON) ìŠ¤í‚¤ë§ˆ: ì¼ë‹¨ ì „ë¶€ Stringìœ¼ë¡œ ë°›ì•„ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
PAYLOAD_JSON_SCHEMA = StructType([
    StructField("event_ts", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("show_id", StringType(), True),
    StructField("session_time", StringType(), True),
    StructField("device", StringType(), True),
])

# JSON payloadì—ì„œ ë°˜ë“œì‹œ í•„ìš”í•œ ì»¬ëŸ¼ë“¤
REQUIRED_COLS = ["event_ts", "user_id", "show_id", "session_time", "device"]


def parse_event_ts(colname="event_ts"):
    """
    ë¬¸ìì—´ timestampë¥¼ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ timestampë¡œ íŒŒì‹±í•œë‹¤.
    - ë‹¤ì–‘í•œ í¬ë§·ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
    - ì‹¤íŒ¨ ì‹œ null (í•˜ìœ„ ë¡œì§ì—ì„œ í•„í„°ë§)
    """
    return F.coalesce(
        F.expr(f"try_to_timestamp({colname}, 'yyyy-MM-dd HH:mm:ss.SSS')"),
        F.expr(f"try_to_timestamp({colname}, 'yyyy-MM-dd HH:mm:ss')"),
        F.expr(f"try_to_timestamp({colname})"),
    )


def snapshot_cutoff_from_data(full_df):
    """
    ğŸ“Œ ë°ì´í„° ê¸°ì¤€ T-1 ìŠ¤ëƒ…ìƒ· í™•ì • ë¡œì§

    ìš´ì˜ í™˜ê²½ ê°€ì •:
      - ì˜¤ëŠ˜(ìµœì‹  ë‚ ì§œ) ë°ì´í„°ëŠ” ì•„ì§ ë¶ˆì™„ì „/ë¶€ë¶„ì¼ ìˆ˜ ìˆìŒ
      - ë”°ë¼ì„œ snapshot_date = max(event_date) - 1 ë¡œ í™•ì •ë³¸ì„ ë§Œë“ ë‹¤.

    ì˜ˆ:
      max(event_date) = 2026-02-14
      â†’ snapshot_date = 2026-02-13
    """
    max_date_df = full_df.agg(F.max("event_date").alias("max_event_date"))
    return max_date_df.select(F.date_sub(F.col("max_event_date"), 1).alias("snapshot_date"))


# =========================================================
# 1) BRONZE LAYER
# =========================================================
# ëª©ì :
#   - ì‚¬ìš©ì/ì½˜í…ì¸  ë§ˆìŠ¤í„° ì›ë³¸ ë¡œë“œ
#   - ê³¼ê±° Delta(íˆìŠ¤í† ë¦¬) ìŠ¤íŠ¸ë¦¼ ë¡œë“œ
#   - Event Hubs Capture(AVRO) ë¡œë“œ
#   - Captureì˜ JSON payload íŒŒì‹±
#   - íŒŒì‹± ì‹¤íŒ¨/í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ë ˆì½”ë“œ ê²©ë¦¬(bad í…Œì´ë¸”)
# =========================================================

@dlt.table(name="dlt_bronze_user", comment="(Bronze) ìœ ì € ë§ˆìŠ¤í„° ì›ë³¸")
def dlt_bronze_user():
    return spark.read.format("csv").option("header", "true").load(USER_SRC)


@dlt.table(name="dlt_bronze_netflix_master", comment="(Bronze) ë„·í”Œë¦­ìŠ¤ ì½˜í…ì¸  ë§ˆìŠ¤í„° ì›ë³¸")
def dlt_bronze_netflix_master():
    return spark.read.format("csv").option("header", "true").load(CONT_SRC)


@dlt.table(name="dlt_bronze_watch_history_stream", comment="(Bronze) ê³¼ê±°(Delta) ì‹œì²­ ë¡œê·¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½ê¸°")
def dlt_bronze_watch_history_stream():
    return (spark.readStream.table(HISTORY_TABLE)
            .select("event_ts", "user_id", "show_id", "session_time", "device"))


@dlt.table(name="dlt_bronze_watch_capture_raw", comment="(Bronze) Event Hubs Capture AVRO raw (ë””ë²„ê¹…ìš©)")
def dlt_bronze_watch_capture_raw():
    """
    Capture ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ì ì¬í•œë‹¤.
    - file_path, ingest_tsë¥¼ ë¶™ì—¬ì„œ ë””ë²„ê¹… ê°€ëŠ¥í•˜ê²Œ í•¨
    """
    raw = (spark.readStream
           .format("cloudFiles")
           .option("cloudFiles.format", "avro")
           .option("cloudFiles.includeExistingFiles", "true")
           .load(WATCH_CAPTURE_SRC))
    return (raw
            .withColumn("file_path", F.col("_metadata.file_path"))
            .withColumn("ingest_ts", F.current_timestamp())
           )


@dlt.table(name="dlt_bronze_watch_event_log_bad", comment="(Bronze) ì‹¤ì‹œê°„ ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨/í•„ìˆ˜ëˆ„ë½ ê²©ë¦¬ í…Œì´ë¸”")
def dlt_bronze_watch_event_log_bad():
    """
    JSON íŒŒì‹± ì‹¤íŒ¨ / payload ë¹„ì •ìƒ / í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ë ˆì½”ë“œë¥¼ ê²©ë¦¬
    - ìš´ì˜ ì‹œ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§/ì›ì¸ ë¶„ì„ì— ì‚¬ìš©
    """
    raw = dlt.read_stream("dlt_bronze_watch_capture_raw")

    raw = raw.withColumn("payload_str", F.col(PAYLOAD_COL).cast("string"))
    parsed = raw.withColumn("j", F.from_json(F.col("payload_str"), PAYLOAD_JSON_SCHEMA, {"mode": "PERMISSIVE"}))

    too_short = F.col("payload_str").isNull() | (F.length(F.col("payload_str")) <= 2)
    parse_fail = F.col("j").isNull()

    cond_required = None
    for c in REQUIRED_COLS:
        expr = F.col(f"j.{c}").isNotNull()
        cond_required = expr if cond_required is None else (cond_required & expr)
    missing_required = ~cond_required

    return (parsed
        .filter(too_short | parse_fail | missing_required)
        .select(
            "payload_str",
            "file_path",
            "ingest_ts",
            F.when(too_short, F.lit("too_short_or_null"))
             .when(parse_fail, F.lit("json_parse_fail"))
             .otherwise(F.lit("missing_required")).alias("bad_reason")
        )
    )


@dlt.table(name="dlt_bronze_watch_event_log", comment="(Bronze) ì‹¤ì‹œê°„ ì‹œì²­ ë¡œê·¸ (ì •ìƒ payloadë§Œ)")
def dlt_bronze_watch_event_log():
    """
    Capture rawì—ì„œ payload(JSON)ë¥¼ íŒŒì‹±í•˜ì—¬ ì •ìƒ ë ˆì½”ë“œë§Œ ì¶”ì¶œ
    """
    raw = dlt.read_stream("dlt_bronze_watch_capture_raw")

    raw = raw.filter(F.col(PAYLOAD_COL).isNotNull())
    with_payload = (raw
        .withColumn("payload_str", F.col(PAYLOAD_COL).cast("string"))
        .filter(F.length(F.col("payload_str")) > 2)
    )

    parsed = with_payload.withColumn(
        "j", F.from_json(F.col("payload_str"), PAYLOAD_JSON_SCHEMA, {"mode": "PERMISSIVE"})
    )
    parsed = parsed.filter(F.col("j").isNotNull())

    cond_required = None
    for c in REQUIRED_COLS:
        expr = F.col(f"j.{c}").isNotNull()
        cond_required = expr if cond_required is None else (cond_required & expr)
    parsed = parsed.filter(cond_required)

    return (parsed.select(
            F.col("j.event_ts").alias("event_ts"),
            F.col("j.user_id").alias("user_id"),
            F.col("j.show_id").alias("show_id"),
            F.col("j.session_time").alias("session_time"),
            F.col("j.device").alias("device"),
            "file_path",
            "ingest_ts"
        )
    )


# =========================================================
# 2) SILVER LAYER
# =========================================================
# ëª©ì :
#   - ì´ë²¤íŠ¸ ë ˆë²¨ ë°ì´í„°ë¥¼ í†µí•©/ì •ì œí•œë‹¤.
#   - ìœ ì €Ã—ì¼ì ë‹¨ìœ„ ì¼ë³„ ì‹œì²­ì‹œê°„ì„ ë§Œë“ ë‹¤.
#   - Full Matrix(ë¯¸ì ‘ì†=0)ë¡œ í™•ì¥í•˜ì—¬ inactivity ê³„ì‚° ê¸°ë°˜ì„ ë§Œë“ ë‹¤.
#
# í•µì‹¬:
#   - history(ê³¼ê±° Delta) + rt(ì‹¤ì‹œê°„ Capture)ë¥¼ union
#   - watermarkë¡œ ì§€ì—° ì´ë²¤íŠ¸ë¥¼ í—ˆìš©
#   - event_tsëŠ” Asia/Seoul ê¸°ì¤€ ë‚ ì§œ(event_date)ë¡œ ë³€í™˜
# =========================================================

@dlt.table(name="dlt_silver_watch_events_all", comment="(Silver, Streaming) ê³¼ê±°(Delta)+ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ í†µí•© (ì§‘ê³„ ì „)")
def dlt_silver_watch_events_all():
    hist = dlt.read_stream("dlt_bronze_watch_history_stream")
    rt   = dlt.read_stream("dlt_bronze_watch_event_log")

    # ì´ë²¤íŠ¸ ë ˆë²¨ í†µí•©
    u = hist.unionByName(
        rt.select("event_ts", "user_id", "show_id", "session_time", "device"),
        allowMissingColumns=True
    )

    # íƒ€ì… ì •ì œ + timestamp íŒŒì‹± + ë‚ ì§œ íŒŒìƒ
    return (u
        .withColumn("event_ts_ts", parse_event_ts("event_ts"))
        .filter(F.col("event_ts_ts").isNotNull())
        .withColumn("user_id", F.col("user_id").cast("int"))
        .withColumn("session_time", F.col("session_time").cast("int"))
        .filter((F.col("session_time") >= 1) & (F.col("session_time") <= 1440))
        .withWatermark("event_ts_ts", "400 days")
        .withColumn("event_date", F.to_date(F.from_utc_timestamp(F.col("event_ts_ts"), "Asia/Seoul")))
    )


@dlt.table(name="dlt_silver_daily_watch_time_rt", comment="(Silver, Streaming) ì‹¤ì‹œê°„(apptesting)ë§Œ ì¼ë³„ ì‹œì²­ì‹œê°„")
def dlt_silver_daily_watch_time_rt():
    """
    (ì˜µì…˜) ì‹¤ì‹œê°„ ë°ì´í„°ë§Œ ë”°ë¡œ ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    - ë©”ì¸ ì§‘ê³„ëŠ” daily_watch_time(ê³¼ê±°+ì‹¤ì‹œê°„ í†µí•©)ì„ ì‚¬ìš©
    """
    df = dlt.read_stream("dlt_bronze_watch_event_log")
    df = (df
          .withColumn("event_ts_ts", parse_event_ts("event_ts"))
          .filter(F.col("event_ts_ts").isNotNull())
          .withWatermark("event_ts_ts", "2 days")
          .withColumn("event_date", F.to_date(F.from_utc_timestamp(F.col("event_ts_ts"), "Asia/Seoul")))
          .withColumn("user_id", F.col("user_id").cast("int"))
          .withColumn("session_time", F.col("session_time").cast("int"))
          .filter((F.col("session_time") >= 1) & (F.col("session_time") <= 1440))
    )
    return (df.groupBy("event_date", "user_id")
              .agg(F.sum("session_time").alias("daily_watch_time"))
              .withColumn("is_active", F.lit(1))
    )


@dlt.table(name="dlt_silver_daily_watch_time", comment="(Silver, Streaming) ê³¼ê±°+ì‹¤ì‹œê°„ í†µí•© ì¼ë³„/ìœ ì €ë³„ ì‹œì²­ì‹œê°„")
def dlt_silver_daily_watch_time():
    """
    ì´ë²¤íŠ¸ í†µí•© í…Œì´ë¸”(dlt_silver_watch_events_all)ì„ ì¼ë³„ ì§‘ê³„
    """
    events = dlt.read_stream("dlt_silver_watch_events_all")
    return (events
        .groupBy("event_date", "user_id")
        .agg(F.sum("session_time").alias("daily_watch_time"))
        .withColumn("is_active", F.lit(1))
    )


@dlt.table(name="dlt_silver_daily_watch_time_full", comment="(Silver) ë¯¸ì ‘ì† ë‚ ì§œ(0ë¶„) í¬í•¨ Full Matrix")
def dlt_silver_daily_watch_time_full():
    """
    Full Matrixë¥¼ ë§Œë“œëŠ” ì´ìœ :
      - ë¯¸ì ‘ì† ì¼ìë¥¼ 0ë¶„/0í™œì„±ìœ¼ë¡œ í¬í•¨í•´ì•¼
        days_since_last_login, inactivity_index ê°™ì€ ì§€í‘œê°€ ì •í™•í•´ì§.
      - ìŠ¤ëƒ…ìƒ·/ë¦¬í…ì…˜/ì „ëµë§¤í•‘ì˜ ê¸°ë°˜ì´ ë¨.
    """
    watch = dlt.read("dlt_silver_daily_watch_time")
    users = dlt.read("dlt_bronze_user").select(
        F.col("user_id").cast("int").alias("user_id"),
        F.to_date("join_date").alias("join_date")
    )

    bounds = watch.agg(F.min("event_date").alias("start"), F.max("event_date").alias("end"))
    dates = bounds.select(
        F.explode(F.sequence(F.col("start"), F.col("end"), F.expr("interval 1 day"))).alias("event_date")
    )

    return (users.crossJoin(dates)
            .filter(F.col("event_date") >= F.col("join_date"))
            .join(watch, on=["event_date", "user_id"], how="left")
            .fillna({"daily_watch_time": 0, "is_active": 0})
    )


# =========================================================
# 3) GOLD LAYER - Snapshot / KPI / Retention
# =========================================================
# ëª©ì :
#   - dlt_gold_user_behavior_snapshot: ìœ ì €ë³„ ì¼ì ìŠ¤ëƒ…ìƒ·(í–‰ë™/ìœ„í—˜)
#   - dlt_gold_service_kpi           : ì„œë¹„ìŠ¤ KPI(DAU/WAU/MAU ë“±)
#   - dlt_gold_churn_risk_kpi        : ì´íƒˆ ìœ„í—˜ KPI(ì¬ê³ /ìœ ì…/íšŒë³µ)
#   - dlt_gold_retention             : ì½”í˜¸íŠ¸ ë¦¬í…ì…˜
#
# ê³µí†µ:
#   - snapshot_cutoff_from_data()ë¡œ T-1 í™•ì •ë³¸ë§Œ ë‚¨ê¸´ë‹¤.
# =========================================================

@dlt.table(
    name="dlt_gold_user_behavior_snapshot",
    comment="""
    (Gold) ìœ ì € í–‰íƒœ ìŠ¤ëƒ…ìƒ· - ë°ì´í„° ê¸°ì¤€ T-1 í™•ì •ë³¸

    ì£¼ìš” ì»¬ëŸ¼:
      - daily_watch_time_min / watch_time_7d_min / watch_time_30d_min
      - active_days_7 / active_days_30
      - days_since_last_login / segment
      - churn_reason / churn_risk_level
      - (ì¶”ê°€) observation_days / frequency_active_days (ì „ëµ ë§¤í•‘ìš©)

    âš  ì´ í…Œì´ë¸”ì€ KPI/Retention/Campaign Targetsì˜ ê¸°ë°˜ í…Œì´ë¸”ì…ë‹ˆë‹¤.
    """
)
def dlt_gold_user_behavior_snapshot():
    df = dlt.read("dlt_silver_daily_watch_time_full")
    cutoff = snapshot_cutoff_from_data(df)

    w7    = Window.partitionBy("user_id").orderBy("event_date").rowsBetween(-6, 0)
    w30   = Window.partitionBy("user_id").orderBy("event_date").rowsBetween(-29, 0)
    w_all = Window.partitionBy("user_id").orderBy("event_date").rowsBetween(Window.unboundedPreceding, 0)

    # 1) ê¸°ë³¸ í–‰ë™ ì§€í‘œ
    base = (df
        .withColumn("watch_time_7d_min",  F.sum("daily_watch_time").over(w7))
        .withColumn("watch_time_30d_min", F.sum("daily_watch_time").over(w30))
        .withColumn("active_days_7",      F.sum("is_active").over(w7).cast("int"))
        .withColumn("active_days_30",     F.sum("is_active").over(w30).cast("int"))
        .withColumn("last_login", F.max(F.when(F.col("is_active") == 1, F.col("event_date"))).over(w_all))
        .withColumn("days_since_last_login", F.datediff("event_date", "last_login"))
        .withColumn(
            "segment",
            F.when(F.col("active_days_30") >= 20, F.lit("Heavy"))
             .when(F.col("active_days_30") >= 5,  F.lit("Mid"))
             .otherwise(F.lit("Light"))
        )
    )

    # 2) ì „ëµ/ì´íƒˆ íŒë‹¨ì„ ìœ„í•œ ë³´ì¡° ì§€í‘œ
    #    - observation_days: ê°€ì…ì¼ ê¸°ì¤€ ëˆ„ì  ê´€ì¸¡ì¼ìˆ˜
    #    - frequency_active_days: ê°€ì… ì´í›„ ëˆ„ì  í™œì„±ì¼ìˆ˜
    base = (base
        .withColumn("observation_days", F.datediff(F.col("event_date"), F.col("join_date")) + F.lit(1))
        .withColumn("frequency_active_days", F.sum("is_active").over(w_all).cast("int"))
        .withColumn(
            "mivt",
            F.when(F.col("frequency_active_days") > 0,
                   F.col("observation_days") / F.col("frequency_active_days").cast("double"))
             .otherwise(F.lit(None).cast("double"))
        )
        .withColumn(
            "inactivity_index",
            F.when((F.col("mivt").isNotNull()) & (F.col("mivt") > 0),
                   F.col("days_since_last_login").cast("double") / F.col("mivt"))
             .otherwise(F.lit(None).cast("double"))
        )
    )

    # 3) churn_reason(ìŠ¤ëƒ…ìƒ· rule) & churn_risk_level
    #    - ëª¨ë¸ì˜ churn_reasonê³¼ ë³„ê°œë¡œ, ìŠ¤ëƒ…ìƒ· ê´€ì  ì›ì¸ì„ ë¶„ë¥˜(ë¦¬í¬íŒ…ìš©)
    ONBOARD_INACT_DAYS = 7
    SILENT_DECAY_MODE = "A"  # A/B ì¤‘ ì„ íƒ

    prechurned_cond = ((F.col("frequency_active_days") == 0) & (F.col("observation_days") >= 60))
    data_gap_cond = ((F.col("frequency_active_days").isin([1, 2])) & (F.col("observation_days") >= 30))
    onboarding_fail_cond = (
        (F.col("observation_days") <= 30) &
        (F.col("frequency_active_days") <= 2) &
        (F.col("days_since_last_login") >= ONBOARD_INACT_DAYS)
    )

    silent_decay_A = ((F.col("active_days_30") >= 6) & (F.col("active_days_7") <= 1))
    silent_decay_B = (
        (F.col("watch_time_30d_min") >= 120) &
        (F.col("watch_time_7d_min") <= (F.col("watch_time_30d_min") * F.lit(0.10)))
    )
    silent_decay_cond = F.when(F.lit(SILENT_DECAY_MODE) == F.lit("B"), silent_decay_B).otherwise(silent_decay_A)

    base = (base
        .withColumn(
            "churn_reason",
            F.when(prechurned_cond, F.lit("prechurned"))
             .when(data_gap_cond, F.lit("data_gap"))
             .when(onboarding_fail_cond, F.lit("onboarding_fail"))
             .when(silent_decay_cond, F.lit("silent_decay"))
             .otherwise(F.lit("normal"))
        )
    )

    base = (base
        .withColumn(
            "churn_risk_level",
            F.when(F.col("inactivity_index").isNull(), F.lit("Active"))
             .when(F.col("inactivity_index") < 1.0,    F.lit("Active"))
             .when(F.col("inactivity_index") < 2.0,    F.lit("Soft Churn"))
             .when(F.col("inactivity_index") < 5.0,    F.lit("Dormant"))
             .otherwise(F.lit("Churned"))
        )
    )

    # 4) ìƒíƒœ ë³´ì •(ë‹¨ìˆœ rule)
    SOFT_DAYS = 14
    CHURN_DAYS_PRE = 60
    dsll = F.coalesce(F.col("days_since_last_login"), F.col("observation_days"))

    base = (base
        .withColumn(
            "churn_risk_level",
            F.when((F.col("churn_reason") == "prechurned") & (dsll >= CHURN_DAYS_PRE), F.lit("Churned"))
             .when((F.col("churn_reason") == "prechurned") & (dsll >= SOFT_DAYS),      F.lit("Dormant"))
             .when((F.col("churn_reason") == "prechurned") & (dsll <  SOFT_DAYS),      F.lit("Soft Churn"))
             .otherwise(F.col("churn_risk_level"))
        )
    )

    # 5) T-1 ì»·ì˜¤í”„ ì ìš© + ì»¬ëŸ¼ ì •ë¦¬
    snap = (base.crossJoin(cutoff)
            .filter(F.col("event_date") <= F.col("snapshot_date"))
            .select(
                "event_date", "user_id", "is_active",
                F.col("daily_watch_time").alias("daily_watch_time_min"),
                "watch_time_7d_min", "watch_time_30d_min",
                "active_days_7", "active_days_30",
                "days_since_last_login", "segment", "churn_risk_level",
                "churn_reason",
                "observation_days",
                "frequency_active_days"
            )
    )

    # (ì„ íƒ) probability_bandë¥¼ ë¶™ì—¬ Lookerì—ì„œ ì¡°ì¸ ì¤„ì´ê¸°
    pred_band = (spark.table(MODEL_PRED_TABLE)
                .select(
                    F.col("event_date").alias("pred_event_date"),
                    F.col("user_id").alias("pred_user_id"),
                    F.col("probability_band").alias("probability_band")
                ))

    snap = (snap.alias("s")
            .join(pred_band.alias("p"),
                  (F.col("s.event_date") == F.col("p.pred_event_date")) &
                  (F.col("s.user_id") == F.col("p.pred_user_id")),
                  "left")
            .select("s.*", "p.probability_band")
    )

    return snap


@dlt.table(name="dlt_gold_service_kpi", comment="(Gold) ì„œë¹„ìŠ¤ KPI - ë°ì´í„° ê¸°ì¤€ T-1 í™•ì •ë³¸")
def dlt_gold_service_kpi():
    full = dlt.read("dlt_silver_daily_watch_time_full")
    cutoff = snapshot_cutoff_from_data(full)

    daily_base = full.groupBy("event_date").agg(
        F.sum("is_active").alias("dau"),
        F.sum("daily_watch_time").alias("total_watch_time_min"),
        F.count("user_id").alias("total_users")
    )

    dates = full.select("event_date").distinct()
    active_logs = full.filter(F.col("is_active") == 1).select("event_date", "user_id")

    wau = (dates.alias("d").join(active_logs.alias("a"),
            (F.col("a.event_date") > F.date_sub(F.col("d.event_date"), 7)) &
            (F.col("a.event_date") <= F.col("d.event_date")))
        .groupBy("d.event_date").agg(F.countDistinct("user_id").alias("wau"))
    )

    mau = (dates.alias("d").join(active_logs.alias("a"),
            (F.col("a.event_date") > F.date_sub(F.col("d.event_date"), 30)) &
            (F.col("a.event_date") <= F.col("d.event_date")))
        .groupBy("d.event_date").agg(F.countDistinct("user_id").alias("mau"))
    )

    result = (daily_base.join(wau, "event_date")
        .join(mau, "event_date")
        .withColumn(
            "avg_watch_min",
            F.round(F.col("total_watch_time_min") / F.when(F.col("dau") == 0, F.lit(None)).otherwise(F.col("dau")), 2)
        )
        .withColumn(
            "active_ratio",
            F.round(F.col("dau") / F.when(F.col("total_users") == 0, F.lit(None)).otherwise(F.col("total_users")), 4)
        )
        .select("event_date", "dau", "wau", "mau", "total_watch_time_min", "avg_watch_min", "active_ratio")
    )

    return (result.crossJoin(cutoff)
            .filter(F.col("event_date") <= F.col("snapshot_date"))
    )


@dlt.table(name="dlt_gold_churn_risk_kpi", comment="(Gold) ì´íƒˆ ìœ„í—˜ KPI - ë°ì´í„° ê¸°ì¤€ T-1 í™•ì •ë³¸")
def dlt_gold_churn_risk_kpi():
    snap = dlt.read("dlt_gold_user_behavior_snapshot").select("event_date", "user_id", "churn_risk_level")

    w_user = Window.partitionBy("user_id").orderBy("event_date")
    s = snap.withColumn("prev_churn_risk_level", F.lag("churn_risk_level", 1).over(w_user))

    is_risk_today = F.col("churn_risk_level").isin(["Dormant", "Churned"])
    is_risk_prev  = F.col("prev_churn_risk_level").isin(["Dormant", "Churned"])

    stock = s.groupBy("event_date").agg(
        F.countDistinct(F.when(F.col("churn_risk_level") == "Active", F.col("user_id"))).alias("active_cnt"),
        F.countDistinct(F.when(F.col("churn_risk_level") == "Soft Churn", F.col("user_id"))).alias("soft_churn_cnt"),
        F.countDistinct(F.when(F.col("churn_risk_level") == "Dormant", F.col("user_id"))).alias("dormant_cnt"),
        F.countDistinct(F.when(F.col("churn_risk_level") == "Churned", F.col("user_id"))).alias("churned_cnt"),
    )

    flow = s.groupBy("event_date").agg(
        F.countDistinct(F.when(is_risk_today, F.col("user_id"))).alias("at_risk_user_cnt"),
        F.countDistinct(F.when(is_risk_today & (~is_risk_prev | F.col("prev_churn_risk_level").isNull()), F.col("user_id"))).alias("at_risk_new_cnt"),
        F.countDistinct(F.when(is_risk_prev & (~is_risk_today), F.col("user_id"))).alias("at_risk_recovered_cnt"),
        F.countDistinct(F.when((F.col("churn_risk_level") == "Churned") & (F.col("prev_churn_risk_level").isin(["Dormant", "Soft Churn"])), F.col("user_id"))).alias("new_churned_cnt")
    )

    return (stock.join(flow, "event_date")
            .select("event_date","at_risk_user_cnt","at_risk_new_cnt","at_risk_recovered_cnt","new_churned_cnt",
                    "active_cnt","soft_churn_cnt","dormant_cnt","churned_cnt")
    )


@dlt.table(name="dlt_gold_retention", comment="(Gold) ë¦¬í…ì…˜ - ë°ì´í„° ê¸°ì¤€ T-1 í™•ì •ë³¸")
def dlt_gold_retention():
    snap = dlt.read("dlt_gold_user_behavior_snapshot").select("event_date","user_id","is_active","segment")

    first_seen = (snap.groupBy("user_id")
                  .agg(F.min("event_date").alias("first_event_date"))
                  .withColumn("cohort_month", F.date_trunc("MONTH", F.col("first_event_date"))))

    monthly_active = (snap
        .filter(F.col("is_active") == 1)
        .select("user_id", F.date_trunc("MONTH", F.col("event_date")).alias("event_month"))
        .distinct()
    )

    ret = (monthly_active.join(first_seen.select("user_id","cohort_month"), "user_id", "inner")
           .withColumn("months_since_join", F.months_between(F.col("event_month"), F.col("cohort_month")).cast("int"))
           .withColumn("is_retained", F.lit(1))
           .select("user_id","cohort_month","event_month","months_since_join","is_retained"))

    cohort_size = first_seen.groupBy("cohort_month").agg(F.countDistinct("user_id").alias("cohort_size"))

    seg_month = (snap
        .withColumn("event_month", F.date_trunc("MONTH", F.col("event_date")))
        .groupBy("user_id","event_month")
        .agg(F.max_by(F.col("segment"), F.col("event_date")).alias("segment_current"))
    )

    return (ret.join(cohort_size, "cohort_month", "left")
            .join(seg_month, ["user_id","event_month"], "left")
            .select("user_id","cohort_month","event_month","months_since_join","cohort_size","is_retained","segment_current")
    )